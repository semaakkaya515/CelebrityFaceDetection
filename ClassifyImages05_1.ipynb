{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                           filepath  \\\n",
      "0              0  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "1              1  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "2              2  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "3              3  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "4              4  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "...          ...                                                ...   \n",
      "2478        2478  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "2479        2479  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "2480        2480  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "2481        2481  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "2482        2482  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "\n",
      "                label  label2  \n",
      "0     Amanda Seyfried       0  \n",
      "1     Amanda Seyfried       0  \n",
      "2     Amanda Seyfried       0  \n",
      "3     Amanda Seyfried       0  \n",
      "4     Amanda Seyfried       0  \n",
      "...               ...     ...  \n",
      "2478       Will Smith       7  \n",
      "2479       Will Smith       7  \n",
      "2480       Will Smith       7  \n",
      "2481       Will Smith       7  \n",
      "2482       Will Smith       7  \n",
      "\n",
      "[2483 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2483"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1 = pd.read_csv(\"C:/Users/Administrator/OneDrive - Florida International University/Desktop/sema_project/data_new04.csv\")\n",
    "# X_1['label2'] = X_1['label'].factorize()[0]\n",
    "# X_1.to_csv(\"C:/Users/Administrator/OneDrive - Florida International University/Desktop/sema_project/data.csv\") \n",
    "\n",
    "print(X_1)\n",
    "len(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                           filepath  \\\n",
      "8            254  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "16          1740  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "18           649  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "26            42  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "59          2472  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "...          ...                                                ...   \n",
      "2464         615  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "2470         940  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "2474        2118  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "2475         501  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "2477         523  C:/Users/Administrator/OneDrive - Florida Inte...   \n",
      "\n",
      "                   label  label2  \n",
      "8        Amanda Seyfried       0  \n",
      "16              Tina Fey       4  \n",
      "18    Sylvester Stallone       1  \n",
      "26       Amanda Seyfried       0  \n",
      "59            Will Smith       7  \n",
      "...                  ...     ...  \n",
      "2464  Sylvester Stallone       1  \n",
      "2470         Talia Shire       2  \n",
      "2474          Vin Diesel       6  \n",
      "2475  Sylvester Stallone       1  \n",
      "2477  Sylvester Stallone       1  \n",
      "\n",
      "[497 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1= X_1.sample(frac=1).reset_index(drop=True)\n",
    "x_train = X_1.sample(frac=.80).reset_index(drop=True)\n",
    "x_test = pd.concat([X_1,x_train]).drop_duplicates(keep=False)\n",
    "print(x_test)\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1=[]\n",
    "y_train1=[]\n",
    "count = 0\n",
    "for _, row in x_train.iterrows():\n",
    "    try: \n",
    "        img = Image.open(str(row['filepath']))\n",
    "        # Get the original size of the image\n",
    "        width, height = img.size\n",
    "\n",
    "        # Calculate the new size while keeping the aspect ratio\n",
    "        if width > height:\n",
    "            new_width = 128\n",
    "            new_height = int(height * (new_width / width))\n",
    "        else:\n",
    "            new_height = 128\n",
    "            new_width = int(width * (new_height / height))\n",
    "\n",
    "        # Resize the image while keeping the aspect ratio\n",
    "        img_resized = img.resize((new_width, new_height))\n",
    "\n",
    "        # Create a new image with black padding if needed\n",
    "        background = Image.new('RGB', (128, 128), (0, 0, 0))\n",
    "        bg_w, bg_h = background.size\n",
    "        offset = ((bg_w - new_width) // 2, (bg_h - new_height) // 2)\n",
    "        background.paste(img_resized, offset)\n",
    "\n",
    "        # Check if the image mode is RGB\n",
    "        if background.mode == \"RGB\":\n",
    "            img_np = np.array(background)\n",
    "            # Append the image and the label\n",
    "            x_train1.append(img_np)\n",
    "            y_train1.append(row['label2'])\n",
    "        else:\n",
    "            # Skip the image and print a message\n",
    "            print(f\"Skipping image {row['filepath']} because it is not RGB\")\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(556, 128, 128, 3)\n",
      "(556,)\n"
     ]
    }
   ],
   "source": [
    "y_train1=np.array(y_train1).astype('float32')\n",
    "x_train1 = np.array(x_train1).astype('float32')/127\n",
    "print(x_train1.shape)\n",
    "print(y_train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 6., 7.], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(556, 8)\n"
     ]
    }
   ],
   "source": [
    "y_train2 = to_categorical(y_train1)\n",
    "print(y_train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(219, 314)\n",
      "(304, 496)\n",
      "(146, 234)\n",
      "(135, 176)\n",
      "(107, 140)\n",
      "(68, 106)\n",
      "(386, 574)\n",
      "(224, 356)\n",
      "(242, 384)\n",
      "(104, 167)\n",
      "(384, 647)\n",
      "(198, 300)\n",
      "(409, 578)\n",
      "(394, 498)\n",
      "(309, 419)\n",
      "(603, 610)\n",
      "(342, 541)\n",
      "(346, 460)\n",
      "(279, 317)\n",
      "(255, 452)\n",
      "(398, 463)\n",
      "(230, 365)\n",
      "(135, 170)\n",
      "(524, 716)\n",
      "(1446, 1737)\n",
      "(234, 294)\n",
      "(500, 750)\n",
      "(130, 198)\n",
      "(300, 399)\n",
      "(147, 234)\n",
      "(298, 451)\n",
      "(1247, 1858)\n",
      "(169, 227)\n",
      "(292, 498)\n",
      "(89, 148)\n",
      "(198, 246)\n",
      "(157, 293)\n",
      "(204, 292)\n",
      "(336, 380)\n",
      "(761, 939)\n",
      "(339, 441)\n",
      "(412, 641)\n",
      "(587, 790)\n",
      "(695, 925)\n",
      "(300, 472)\n",
      "(246, 413)\n",
      "(267, 360)\n",
      "(242, 315)\n",
      "(308, 417)\n",
      "(117, 187)\n",
      "(397, 635)\n",
      "(193, 313)\n",
      "(808, 1339)\n",
      "(119, 186)\n",
      "(297, 515)\n",
      "(279, 426)\n",
      "(240, 286)\n",
      "(207, 319)\n",
      "(54, 68)\n",
      "(402, 257)\n",
      "(500, 300)\n",
      "(124, 202)\n",
      "(185, 299)\n",
      "(128, 193)\n",
      "(329, 493)\n",
      "(305, 518)\n",
      "(356, 431)\n",
      "(278, 406)\n",
      "(442, 629)\n",
      "(720, 900)\n",
      "(157, 245)\n",
      "(283, 328)\n",
      "(266, 321)\n",
      "(255, 368)\n",
      "(567, 955)\n",
      "(459, 657)\n",
      "(138, 189)\n",
      "(94, 127)\n",
      "(151, 260)\n",
      "(159, 249)\n",
      "(266, 405)\n",
      "(340, 427)\n",
      "(277, 329)\n",
      "(175, 230)\n",
      "(882, 1559)\n",
      "(284, 528)\n",
      "(230, 378)\n",
      "(403, 533)\n",
      "(551, 938)\n",
      "(255, 406)\n",
      "(131, 205)\n",
      "(336, 526)\n",
      "(152, 233)\n",
      "(260, 416)\n",
      "(427, 703)\n",
      "(268, 402)\n",
      "(1024, 768)\n",
      "(174, 270)\n",
      "(366, 526)\n",
      "(1068, 1755)\n",
      "(1016, 1644)\n",
      "(316, 421)\n",
      "(860, 1400)\n",
      "(120, 170)\n",
      "(220, 295)\n",
      "(200, 200)\n",
      "(546, 970)\n",
      "(210, 248)\n",
      "(1109, 1000)\n",
      "(126, 241)\n",
      "(231, 252)\n",
      "(122, 191)\n",
      "(542, 828)\n",
      "(237, 364)\n",
      "(169, 252)\n",
      "(225, 408)\n",
      "(237, 475)\n",
      "(666, 911)\n",
      "(170, 259)\n",
      "(435, 607)\n",
      "(1047, 1506)\n",
      "(242, 388)\n",
      "(100, 152)\n",
      "(349, 480)\n",
      "(885, 971)\n",
      "(492, 657)\n",
      "(263, 389)\n",
      "(178, 317)\n",
      "(961, 1418)\n",
      "(365, 439)\n",
      "(318, 694)\n",
      "(69, 91)\n",
      "(260, 417)\n",
      "(216, 350)\n",
      "(158, 225)\n",
      "(235, 345)\n",
      "(254, 381)\n",
      "(127, 142)\n",
      "(150, 200)\n",
      "(352, 512)\n",
      "(600, 779)\n",
      "(169, 280)\n",
      "(294, 439)\n",
      "(447, 689)\n",
      "(192, 266)\n",
      "(181, 283)\n",
      "(540, 751)\n",
      "(651, 1063)\n",
      "(75, 99)\n",
      "(247, 386)\n",
      "(634, 634)\n"
     ]
    }
   ],
   "source": [
    "x_test1=[]\n",
    "y_test1=[]\n",
    "\n",
    "for _, row in x_test.iterrows():\n",
    "    try: \n",
    "        img = Image.open(str(row['filepath']))\n",
    "        # Get the original size of the image\n",
    "        width, height = img.size\n",
    "        print(img.size)\n",
    "        # Calculate the new size while keeping the aspect ratio\n",
    "        if width > height:\n",
    "            new_width = 128\n",
    "            new_height = int(height * (new_width / width))\n",
    "        else:\n",
    "            new_height = 128\n",
    "            new_width = int(width * (new_height / height))\n",
    "\n",
    "        # Resize the image while keeping the aspect ratio\n",
    "        img_resized = img.resize((new_width, new_height))\n",
    "\n",
    "        # Create a new image with black padding if needed\n",
    "        background = Image.new('RGB', (128, 128), (0, 0, 0))\n",
    "        bg_w, bg_h = background.size\n",
    "        offset = ((bg_w - new_width) // 2, (bg_h - new_height) // 2)\n",
    "        background.paste(img_resized, offset)\n",
    "\n",
    "        # Check if the image mode is RGB\n",
    "        if background.mode == \"RGB\":\n",
    "            # print(img.size)\n",
    "            img_np = np.array(background)\n",
    "            # Append the image and the label\n",
    "            x_test1.append(img_np)\n",
    "            y_test1.append(row['label2'])\n",
    "        else:\n",
    "            # Skip the image and print a message\n",
    "            print(f\"Skipping image {row['filepath']} because it is not RGB\")\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151, 128, 128, 3)\n",
      "(151,)\n"
     ]
    }
   ],
   "source": [
    "y_test1=np.array(y_test1).astype('float32')\n",
    "x_test1=np.array(x_test1).astype('float32')/127\n",
    "print(x_test1.shape)\n",
    "print(y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, AveragePooling2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "#1st block\n",
    "# model.add(data_augmentation)\n",
    "model.add(Conv2D(96, (11,11), strides=(4,4), activation='relu', input_shape=(128, 128, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#2nd block\n",
    "model.add(Conv2D(256, (5,5), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#3rd block\n",
    "model.add(Conv2D(384,(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(384,(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(256,(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#4th block\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096,input_shape=(128*128*3,),activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4096,input_shape=(4096,),activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(8,input_shape=(4096,),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "checkpoint_path = \"training/best_model.h5\"\n",
    "\n",
    "# Create the ModelCheckpoint callback\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',  # Metric to monitor for saving the best model\n",
    "    save_best_only=True, # Save only the best model, not every epoch\n",
    "    save_weights_only=False,  # Save the entire model, not just weights\n",
    "    verbose=1  # Display progress during saving\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 4.9929 - accuracy: 0.4414\n",
      "Epoch 1: val_loss improved from inf to 113.39303, saving model to training\\best_model.h5\n",
      "7/7 [==============================] - 3s 360ms/step - loss: 4.9929 - accuracy: 0.4414 - val_loss: 113.3930 - val_accuracy: 0.4375\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.3072 - accuracy: 0.5203\n",
      "Epoch 2: val_loss did not improve from 113.39303\n",
      "7/7 [==============================] - 2s 311ms/step - loss: 1.3072 - accuracy: 0.5203 - val_loss: 336.8259 - val_accuracy: 0.4375\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.1976 - accuracy: 0.5586\n",
      "Epoch 3: val_loss did not improve from 113.39303\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.1976 - accuracy: 0.5586 - val_loss: 201.0515 - val_accuracy: 0.4375\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.8989 - accuracy: 0.6014\n",
      "Epoch 4: val_loss improved from 113.39303 to 77.16504, saving model to training\\best_model.h5\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.8989 - accuracy: 0.6014 - val_loss: 77.1650 - val_accuracy: 0.4375\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.8492 - accuracy: 0.6261\n",
      "Epoch 5: val_loss improved from 77.16504 to 75.69563, saving model to training\\best_model.h5\n",
      "7/7 [==============================] - 2s 343ms/step - loss: 0.8492 - accuracy: 0.6261 - val_loss: 75.6956 - val_accuracy: 0.4375\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.8732 - accuracy: 0.5923\n",
      "Epoch 6: val_loss improved from 75.69563 to 42.85894, saving model to training\\best_model.h5\n",
      "7/7 [==============================] - 2s 342ms/step - loss: 0.8732 - accuracy: 0.5923 - val_loss: 42.8589 - val_accuracy: 0.4375\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.8044 - accuracy: 0.6216\n",
      "Epoch 7: val_loss improved from 42.85894 to 37.99191, saving model to training\\best_model.h5\n",
      "7/7 [==============================] - 2s 345ms/step - loss: 0.8044 - accuracy: 0.6216 - val_loss: 37.9919 - val_accuracy: 0.4375\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.8020 - accuracy: 0.6959\n",
      "Epoch 8: val_loss improved from 37.99191 to 17.56414, saving model to training\\best_model.h5\n",
      "7/7 [==============================] - 2s 355ms/step - loss: 0.8020 - accuracy: 0.6959 - val_loss: 17.5641 - val_accuracy: 0.4375\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.7401 - accuracy: 0.6712\n",
      "Epoch 9: val_loss improved from 17.56414 to 6.43908, saving model to training\\best_model.h5\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.7401 - accuracy: 0.6712 - val_loss: 6.4391 - val_accuracy: 0.4554\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6513 - accuracy: 0.7162\n",
      "Epoch 10: val_loss did not improve from 6.43908\n",
      "7/7 [==============================] - 2s 313ms/step - loss: 0.6513 - accuracy: 0.7162 - val_loss: 6.7046 - val_accuracy: 0.4375\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.6035 - accuracy: 0.7365\n",
      "Epoch 11: val_loss did not improve from 6.43908\n",
      "7/7 [==============================] - 2s 311ms/step - loss: 0.6035 - accuracy: 0.7365 - val_loss: 7.0726 - val_accuracy: 0.4375\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5574 - accuracy: 0.7635\n",
      "Epoch 12: val_loss improved from 6.43908 to 4.51717, saving model to training\\best_model.h5\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.5574 - accuracy: 0.7635 - val_loss: 4.5172 - val_accuracy: 0.4911\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4854 - accuracy: 0.7905\n",
      "Epoch 13: val_loss improved from 4.51717 to 2.45452, saving model to training\\best_model.h5\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.4854 - accuracy: 0.7905 - val_loss: 2.4545 - val_accuracy: 0.4732\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5631 - accuracy: 0.7770\n",
      "Epoch 14: val_loss did not improve from 2.45452\n",
      "7/7 [==============================] - 2s 313ms/step - loss: 0.5631 - accuracy: 0.7770 - val_loss: 16.2905 - val_accuracy: 0.4375\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4713 - accuracy: 0.8266\n",
      "Epoch 15: val_loss did not improve from 2.45452\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 0.4713 - accuracy: 0.8266 - val_loss: 9.1511 - val_accuracy: 0.4375\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4372 - accuracy: 0.8288\n",
      "Epoch 16: val_loss did not improve from 2.45452\n",
      "7/7 [==============================] - 2s 308ms/step - loss: 0.4372 - accuracy: 0.8288 - val_loss: 9.6093 - val_accuracy: 0.4375\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3335 - accuracy: 0.8468\n",
      "Epoch 17: val_loss did not improve from 2.45452\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 0.3335 - accuracy: 0.8468 - val_loss: 4.5976 - val_accuracy: 0.4464\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3726 - accuracy: 0.8874\n",
      "Epoch 18: val_loss did not improve from 2.45452\n",
      "7/7 [==============================] - 2s 315ms/step - loss: 0.3726 - accuracy: 0.8874 - val_loss: 9.1853 - val_accuracy: 0.4375\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2745 - accuracy: 0.9099\n",
      "Epoch 19: val_loss did not improve from 2.45452\n",
      "7/7 [==============================] - 2s 314ms/step - loss: 0.2745 - accuracy: 0.9099 - val_loss: 15.8229 - val_accuracy: 0.4375\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2826 - accuracy: 0.8941\n",
      "Epoch 20: val_loss did not improve from 2.45452\n",
      "7/7 [==============================] - 2s 318ms/step - loss: 0.2826 - accuracy: 0.8941 - val_loss: 7.5419 - val_accuracy: 0.4643\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1547 - accuracy: 0.9257\n",
      "Epoch 21: val_loss improved from 2.45452 to 0.70306, saving model to training\\best_model.h5\n",
      "7/7 [==============================] - 2s 361ms/step - loss: 0.1547 - accuracy: 0.9257 - val_loss: 0.7031 - val_accuracy: 0.7857\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1952 - accuracy: 0.9369\n",
      "Epoch 22: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 312ms/step - loss: 0.1952 - accuracy: 0.9369 - val_loss: 1.5059 - val_accuracy: 0.5446\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 0.9032\n",
      "Epoch 23: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 314ms/step - loss: 0.2164 - accuracy: 0.9032 - val_loss: 8.0862 - val_accuracy: 0.4732\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2405 - accuracy: 0.9122\n",
      "Epoch 24: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 316ms/step - loss: 0.2405 - accuracy: 0.9122 - val_loss: 8.5227 - val_accuracy: 0.4821\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1836 - accuracy: 0.9212\n",
      "Epoch 25: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 317ms/step - loss: 0.1836 - accuracy: 0.9212 - val_loss: 5.2536 - val_accuracy: 0.5536\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1512 - accuracy: 0.9527\n",
      "Epoch 26: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 318ms/step - loss: 0.1512 - accuracy: 0.9527 - val_loss: 7.6702 - val_accuracy: 0.5804\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2605 - accuracy: 0.9167\n",
      "Epoch 27: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 315ms/step - loss: 0.2605 - accuracy: 0.9167 - val_loss: 7.7104 - val_accuracy: 0.4732\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1441 - accuracy: 0.9527\n",
      "Epoch 28: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 315ms/step - loss: 0.1441 - accuracy: 0.9527 - val_loss: 2.0454 - val_accuracy: 0.4732\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1512 - accuracy: 0.9414\n",
      "Epoch 29: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 312ms/step - loss: 0.1512 - accuracy: 0.9414 - val_loss: 2.8440 - val_accuracy: 0.3304\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9662\n",
      "Epoch 30: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 314ms/step - loss: 0.1571 - accuracy: 0.9662 - val_loss: 3.1058 - val_accuracy: 0.3571\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9752\n",
      "Epoch 31: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 317ms/step - loss: 0.0880 - accuracy: 0.9752 - val_loss: 1.3289 - val_accuracy: 0.5804\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9842\n",
      "Epoch 32: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0544 - accuracy: 0.9842 - val_loss: 6.2171 - val_accuracy: 0.4018\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9730\n",
      "Epoch 33: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 313ms/step - loss: 0.0863 - accuracy: 0.9730 - val_loss: 3.1846 - val_accuracy: 0.5446\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 0.9459\n",
      "Epoch 34: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.1782 - accuracy: 0.9459 - val_loss: 5.5823 - val_accuracy: 0.4643\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9595\n",
      "Epoch 35: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 314ms/step - loss: 0.1194 - accuracy: 0.9595 - val_loss: 3.0693 - val_accuracy: 0.5089\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9730\n",
      "Epoch 36: val_loss did not improve from 0.70306\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0777 - accuracy: 0.9730 - val_loss: 1.2783 - val_accuracy: 0.6429\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9730\n",
      "Epoch 37: val_loss improved from 0.70306 to 0.57043, saving model to training\\best_model.h5\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.0683 - accuracy: 0.9730 - val_loss: 0.5704 - val_accuracy: 0.8482\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9752\n",
      "Epoch 38: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 314ms/step - loss: 0.0747 - accuracy: 0.9752 - val_loss: 0.8206 - val_accuracy: 0.8125\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9842\n",
      "Epoch 39: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 315ms/step - loss: 0.0468 - accuracy: 0.9842 - val_loss: 0.7172 - val_accuracy: 0.8214\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9932\n",
      "Epoch 40: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 316ms/step - loss: 0.0297 - accuracy: 0.9932 - val_loss: 1.1906 - val_accuracy: 0.7679\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9977\n",
      "Epoch 41: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 317ms/step - loss: 0.0120 - accuracy: 0.9977 - val_loss: 2.1401 - val_accuracy: 0.7143\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9887\n",
      "Epoch 42: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 0.0380 - accuracy: 0.9887 - val_loss: 1.8920 - val_accuracy: 0.8036\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9910\n",
      "Epoch 43: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 318ms/step - loss: 0.0354 - accuracy: 0.9910 - val_loss: 0.6998 - val_accuracy: 0.8661\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9797\n",
      "Epoch 44: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0816 - accuracy: 0.9797 - val_loss: 1.4294 - val_accuracy: 0.8482\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.9572\n",
      "Epoch 45: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 0.1933 - accuracy: 0.9572 - val_loss: 2.8900 - val_accuracy: 0.5625\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9617\n",
      "Epoch 46: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 0.1170 - accuracy: 0.9617 - val_loss: 4.1040 - val_accuracy: 0.5357\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9775\n",
      "Epoch 47: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 333ms/step - loss: 0.0747 - accuracy: 0.9775 - val_loss: 0.8728 - val_accuracy: 0.8661\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9865\n",
      "Epoch 48: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 333ms/step - loss: 0.0439 - accuracy: 0.9865 - val_loss: 4.5841 - val_accuracy: 0.6071\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9932\n",
      "Epoch 49: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 336ms/step - loss: 0.0264 - accuracy: 0.9932 - val_loss: 7.2836 - val_accuracy: 0.5268\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9955\n",
      "Epoch 50: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 0.0108 - accuracy: 0.9955 - val_loss: 6.5387 - val_accuracy: 0.5982\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9955\n",
      "Epoch 51: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 0.0158 - accuracy: 0.9955 - val_loss: 3.0935 - val_accuracy: 0.6964\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9955\n",
      "Epoch 52: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 0.0177 - accuracy: 0.9955 - val_loss: 5.3620 - val_accuracy: 0.7143\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 53: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 6.5606 - val_accuracy: 0.6964\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 54: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 6.4945 - val_accuracy: 0.6964\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000  \n",
      "Epoch 55: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 333ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.6420 - val_accuracy: 0.7143\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 8.7494e-04 - accuracy: 1.0000\n",
      "Epoch 56: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 334ms/step - loss: 8.7494e-04 - accuracy: 1.0000 - val_loss: 2.4908 - val_accuracy: 0.7679\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 7.8088e-04 - accuracy: 1.0000\n",
      "Epoch 57: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 327ms/step - loss: 7.8088e-04 - accuracy: 1.0000 - val_loss: 1.7683 - val_accuracy: 0.8036\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 0.9977\n",
      "Epoch 58: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 336ms/step - loss: 0.0046 - accuracy: 0.9977 - val_loss: 2.3174 - val_accuracy: 0.8036\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9955\n",
      "Epoch 59: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 330ms/step - loss: 0.0064 - accuracy: 0.9955 - val_loss: 3.7233 - val_accuracy: 0.7500\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9977  \n",
      "Epoch 60: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 334ms/step - loss: 0.0027 - accuracy: 0.9977 - val_loss: 3.4728 - val_accuracy: 0.7500\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9977\n",
      "Epoch 61: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 335ms/step - loss: 0.0076 - accuracy: 0.9977 - val_loss: 1.3576 - val_accuracy: 0.8482\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9977\n",
      "Epoch 62: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 331ms/step - loss: 0.0035 - accuracy: 0.9977 - val_loss: 2.8237 - val_accuracy: 0.7857\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 63: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 337ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 4.0534 - val_accuracy: 0.6786\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 64: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.0809 - val_accuracy: 0.7500\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 65: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.7624 - val_accuracy: 0.7679\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 66: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 332ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.5932 - val_accuracy: 0.7946\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9977\n",
      "Epoch 67: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 328ms/step - loss: 0.0100 - accuracy: 0.9977 - val_loss: 2.6931 - val_accuracy: 0.8036\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9887\n",
      "Epoch 68: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 0.0409 - accuracy: 0.9887 - val_loss: 1.4782 - val_accuracy: 0.8571\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9752\n",
      "Epoch 69: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 333ms/step - loss: 0.1109 - accuracy: 0.9752 - val_loss: 1.4201 - val_accuracy: 0.8125\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9550\n",
      "Epoch 70: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 330ms/step - loss: 0.1315 - accuracy: 0.9550 - val_loss: 4.2712 - val_accuracy: 0.4464\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1519 - accuracy: 0.9482\n",
      "Epoch 71: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 330ms/step - loss: 0.1519 - accuracy: 0.9482 - val_loss: 6.4730 - val_accuracy: 0.3929\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0894 - accuracy: 0.9707\n",
      "Epoch 72: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 0.0894 - accuracy: 0.9707 - val_loss: 5.9848 - val_accuracy: 0.4375\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 0.9550\n",
      "Epoch 73: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 331ms/step - loss: 0.1716 - accuracy: 0.9550 - val_loss: 1.1060 - val_accuracy: 0.8036\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9707\n",
      "Epoch 74: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 0.1216 - accuracy: 0.9707 - val_loss: 4.1441 - val_accuracy: 0.5000\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9572\n",
      "Epoch 75: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 331ms/step - loss: 0.1068 - accuracy: 0.9572 - val_loss: 3.9117 - val_accuracy: 0.5446\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9865\n",
      "Epoch 76: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 0.0620 - accuracy: 0.9865 - val_loss: 4.6012 - val_accuracy: 0.5625\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9842\n",
      "Epoch 77: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 330ms/step - loss: 0.0410 - accuracy: 0.9842 - val_loss: 4.3486 - val_accuracy: 0.5089\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9910\n",
      "Epoch 78: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0196 - accuracy: 0.9910 - val_loss: 2.5029 - val_accuracy: 0.5893\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9977\n",
      "Epoch 79: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 339ms/step - loss: 0.0107 - accuracy: 0.9977 - val_loss: 1.6866 - val_accuracy: 0.7143\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 0.9977\n",
      "Epoch 80: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 344ms/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 0.7328 - val_accuracy: 0.8482\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9932  \n",
      "Epoch 81: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 325ms/step - loss: 0.0292 - accuracy: 0.9932 - val_loss: 2.3189 - val_accuracy: 0.6429\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9842\n",
      "Epoch 82: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 328ms/step - loss: 0.0494 - accuracy: 0.9842 - val_loss: 11.0173 - val_accuracy: 0.7054\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9640\n",
      "Epoch 83: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 0.0969 - accuracy: 0.9640 - val_loss: 2.1792 - val_accuracy: 0.8304\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9865\n",
      "Epoch 84: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 328ms/step - loss: 0.0463 - accuracy: 0.9865 - val_loss: 1.0239 - val_accuracy: 0.8214\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9752\n",
      "Epoch 85: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 328ms/step - loss: 0.0853 - accuracy: 0.9752 - val_loss: 4.3528 - val_accuracy: 0.7589\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9887\n",
      "Epoch 86: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 328ms/step - loss: 0.0359 - accuracy: 0.9887 - val_loss: 3.8792 - val_accuracy: 0.6875\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9910\n",
      "Epoch 87: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 330ms/step - loss: 0.0241 - accuracy: 0.9910 - val_loss: 6.1109 - val_accuracy: 0.5357\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9820\n",
      "Epoch 88: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 0.0652 - accuracy: 0.9820 - val_loss: 5.3033 - val_accuracy: 0.5268\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9887\n",
      "Epoch 89: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 327ms/step - loss: 0.0295 - accuracy: 0.9887 - val_loss: 1.4655 - val_accuracy: 0.8214\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9820\n",
      "Epoch 90: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 325ms/step - loss: 0.0442 - accuracy: 0.9820 - val_loss: 0.7944 - val_accuracy: 0.8929\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9955\n",
      "Epoch 91: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 332ms/step - loss: 0.0131 - accuracy: 0.9955 - val_loss: 0.9669 - val_accuracy: 0.8661\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9932\n",
      "Epoch 92: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 337ms/step - loss: 0.0127 - accuracy: 0.9932 - val_loss: 0.7694 - val_accuracy: 0.9018\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9977\n",
      "Epoch 93: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0053 - accuracy: 0.9977 - val_loss: 0.7809 - val_accuracy: 0.9286\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9910\n",
      "Epoch 94: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 343ms/step - loss: 0.0190 - accuracy: 0.9910 - val_loss: 2.6795 - val_accuracy: 0.6161\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9865\n",
      "Epoch 95: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 318ms/step - loss: 0.0648 - accuracy: 0.9865 - val_loss: 1.6244 - val_accuracy: 0.7946\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9910\n",
      "Epoch 96: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 315ms/step - loss: 0.0299 - accuracy: 0.9910 - val_loss: 1.6716 - val_accuracy: 0.8482\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9910\n",
      "Epoch 97: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0136 - accuracy: 0.9910 - val_loss: 1.8302 - val_accuracy: 0.8125\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9977\n",
      "Epoch 98: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 312ms/step - loss: 0.0095 - accuracy: 0.9977 - val_loss: 1.5011 - val_accuracy: 0.8304\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 0.9977\n",
      "Epoch 99: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0048 - accuracy: 0.9977 - val_loss: 1.0633 - val_accuracy: 0.8839\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9910\n",
      "Epoch 100: val_loss did not improve from 0.57043\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0125 - accuracy: 0.9910 - val_loss: 0.7951 - val_accuracy: 0.8393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2778c2b19f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train1, y_train2, epochs=100, batch_size=64, validation_split=0.2, callbacks=[checkpoint_callback])\n",
    "# model.fit(x_train1, y_train2, epochs=100, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model.history.history['loss'])\n",
    "plt.plot(model.history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.savefig('loss_fucntion0.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model = tf.keras.models.load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# # Save the entire model in SavedModel format\n",
    "# model.save('my_model5_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model back\n",
    "# load_model = tf.keras.models.load_model('my_model5_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151, 8)\n"
     ]
    }
   ],
   "source": [
    "y_test2 = to_categorical(y_test1)\n",
    "print(y_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 31ms/step - loss: 1.0859 - accuracy: 0.8411\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = model.evaluate(x_test1, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 31ms/step\n",
      "[[1.81307521e-04 2.11016959e-04 2.21191644e-04 ... 3.41715738e-02\n",
      "  5.98747551e-01 3.66137058e-01]\n",
      " [1.29174634e-18 8.52120191e-18 2.47904499e-18 ... 8.48766888e-08\n",
      "  9.99999881e-01 1.76831989e-08]\n",
      " [5.00766502e-04 5.47509873e-04 6.84497645e-04 ... 7.79066235e-02\n",
      "  8.26544225e-01 9.28589255e-02]\n",
      " ...\n",
      " [3.72332547e-12 7.56761667e-12 9.08716852e-12 ... 6.22859923e-04\n",
      "  9.99375045e-01 2.12981695e-06]\n",
      " [1.59611102e-10 1.01108615e-10 4.31212260e-10 ... 3.60550730e-05\n",
      "  1.27721846e-03 9.98686731e-01]\n",
      " [4.24688869e-14 1.40482081e-13 1.17662065e-13 ... 1.17299605e-04\n",
      "  9.90164936e-01 9.71778575e-03]]\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(x_test1) \n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 6 6 5 5 7 5 5 6 5 5 6 5 7 6 6 7 6 5 5 5 7 5 5 5 5 5 5 5 5 6 6 6 6 7\n",
      " 7 6 6 5 7 7 7 5 6 6 5 6 7 7 6 6 6 6 6 7 5 7 5 7 5 5 6 5 6 5 5 5 7 6 7 5 5\n",
      " 5 5 5 5 6 5 5 5 6 6 6 5 7 6 7 6 5 6 6 6 7 6 5 6 5 6 7 5 5 5 5 5 7 6 5 6 6\n",
      " 6 7 6 6 6 7 5 5 6 5 6 5 7 5 5 5 7 6 5 7 6 6 5 6 5 5 6 5 6 5 5 5 7 7 6 6 5\n",
      " 6 7 6]\n"
     ]
    }
   ],
   "source": [
    "classes=np.argmax(predicted,axis=1)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = [5, 6, 7] # use the numbers as labels for the confusion matrix\n",
    "cm = confusion_matrix(y_test1, classes, labels=labels)\n",
    "df_cm = pd.DataFrame(cm, labels, labels)\n",
    "ax = sn.heatmap(df_cm, annot=True, annot_kws={'size': 16}, square=True, cbar=False, fmt='g')\n",
    "ax.set_ylim(0, 3)\n",
    "plt.xlabel('Predicted') \n",
    "plt.ylabel('Actual')\n",
    "ax.invert_yaxis() #optional\n",
    "\n",
    "# set the tick labels of the x-axis and y-axis to show the desired labels\n",
    "plt.xticks([0.5, 1.5, 2.5], ['UT', 'VD', 'WS']) # set the x-axis tick locations and labels\n",
    "plt.yticks([0.5, 1.5, 2.5], ['UT', 'VD', 'WS']) # set the y-axis tick locations and labels\n",
    "\n",
    "plt.savefig('heatmap0.pdf', format='pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b7a87145c0c489198195dcc08ad54a4d38b2d0819abd05b0d500963bab37a76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
